# Task 01 – Fine-Tuning GPT-2 for Custom Text Generation  
**Track Code:** GA | **Repo Name:** PRODIGY_GA_01

This project demonstrates how to fine-tune the **GPT-2** language model to generate text in a custom style. The model was trained on a text dataset using Hugging Face’s Transformers library and evaluated via prompt-based generation.

---

## 🚀 Key Objectives

- Understand how GPT-2 works as a transformer-based autoregressive model  
- Load and preprocess a custom `.txt` corpus for language modeling  
- Fine-tune the pretrained GPT-2 model using Hugging Face Trainer API  
- Generate new text based on prompts using the trained model

---

## 💡 What I Learned

- Tokenization and attention masking with GPT-2  
- Optimizing transformer models using Hugging Face's `Trainer`  
- Prompt engineering and controlling output length  
- Practical understanding of language modeling fundamentals

---

## 🛠️ Tools & Frameworks

- Python  
- Google Colab  
- Hugging Face `transformers`, `datasets`, `tokenizers`  
- PyTorch backend  
- Custom dataset for model training  

---

## 📁 Files

- `Prodigy_Task1.ipy - Colab.html` – Exported Colab notebook  
- `README.md` – Task summary and insights

---

## 📌 Internship Information

> ✅ **Completed as part of:**  
> Prodigy InfoTech – Generative AI Internship  
> **Timeline:** June–July 2025  
>  
> #ProdigyInfoTech #GenerativeAI #GPT2 #AIInternship #TextGeneration #Transformers
