# Task 01 â€“ Fine-Tuning GPT-2 for Custom Text Generation  
**Track Code:** GA | **Repo Name:** PRODIGY_GA_01

This project demonstrates how to fine-tune the **GPT-2** language model to generate text in a custom style. The model was trained on a text dataset using Hugging Faceâ€™s Transformers library and evaluated via prompt-based generation.

---

## ðŸš€ Key Objectives

- Understand how GPT-2 works as a transformer-based autoregressive model  
- Load and preprocess a custom `.txt` corpus for language modeling  
- Fine-tune the pretrained GPT-2 model using Hugging Face Trainer API  
- Generate new text based on prompts using the trained model

---

## ðŸ’¡ What I Learned

- Tokenization and attention masking with GPT-2  
- Optimizing transformer models using Hugging Face's `Trainer`  
- Prompt engineering and controlling output length  
- Practical understanding of language modeling fundamentals

---

## ðŸ› ï¸ Tools & Frameworks

- Python  
- Google Colab  
- Hugging Face `transformers`, `datasets`, `tokenizers`  
- PyTorch backend  
- Custom dataset for model training  

---

## ðŸ“ Files

- `Prodigy_Task1.ipy - Colab.html` â€“ Exported Colab notebook  
- `README.md` â€“ Task summary and insights

---

## ðŸ“Œ Internship Information

> âœ… **Completed as part of:**  
> Prodigy InfoTech â€“ Generative AI Internship  
> **Timeline:** Juneâ€“July 2025  
>  
> #ProdigyInfoTech #GenerativeAI #GPT2 #AIInternship #TextGeneration #Transformers
